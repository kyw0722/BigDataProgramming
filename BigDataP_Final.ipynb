{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "import pandas_datareader as pdr\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "63/63 [==============================] - 1s 869us/step - loss: 0.7268 - acc: 0.4990\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.7266 - acc: 0.4851\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.7050 - acc: 0.5315\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 0s 981us/step - loss: 0.7067 - acc: 0.5071\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.7078 - acc: 0.5093\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.7100 - acc: 0.4742\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.7010 - acc: 0.5008\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.6984 - acc: 0.4996\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.7000 - acc: 0.5059\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6972 - acc: 0.5003\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 0s 981us/step - loss: 0.6960 - acc: 0.5159\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 0s 981us/step - loss: 0.6935 - acc: 0.5246\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6953 - acc: 0.5114\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.6957 - acc: 0.5081\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6960 - acc: 0.5063\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.6964 - acc: 0.4865\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6943 - acc: 0.5038\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6941 - acc: 0.5096\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6955 - acc: 0.5005\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.6939 - acc: 0.5224\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6933 - acc: 0.5224\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 0s 949us/step - loss: 0.6933 - acc: 0.5008\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6941 - acc: 0.5107\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6941 - acc: 0.5019\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6923 - acc: 0.5173\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6933 - acc: 0.5285\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 0s 869us/step - loss: 0.6944 - acc: 0.5110\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6921 - acc: 0.5239\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6938 - acc: 0.5051\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 0s 853us/step - loss: 0.6941 - acc: 0.5050\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6947 - acc: 0.4968\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 0s 869us/step - loss: 0.6916 - acc: 0.5267\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.6926 - acc: 0.5179\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.6940 - acc: 0.5127\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 0s 981us/step - loss: 0.6938 - acc: 0.5067\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6930 - acc: 0.5163\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6938 - acc: 0.5123\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6937 - acc: 0.5079\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6922 - acc: 0.5251\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.6929 - acc: 0.5138\n",
      "Epoch 41/50\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.6906 - acc: 0.5481\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.6929 - acc: 0.5216\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.6947 - acc: 0.4992\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6921 - acc: 0.5216\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6915 - acc: 0.5344\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6926 - acc: 0.5190\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6934 - acc: 0.5203\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.6928 - acc: 0.5165\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6927 - acc: 0.5184\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6915 - acc: 0.5385\n",
      "0.5487077534791253\n"
     ]
    }
   ],
   "source": [
    "#RNN\n",
    "\n",
    "df = pdr.get_data_yahoo(\"AAPL\", \"2010-11-01\", \"2020-11-01\")   # pandas datareader로 주가 데이터를 불러옴\n",
    "df[\"Diff\"] = df.Close.diff()                                  # 당일 종가 - 전일 종가\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()                      # rolling(x).mean으로 x일 간의 주가 평균값(이동평균선을 그리기 위함)을 구함\n",
    "df[\"Force_Index\"] = df.Close * df.Volume                      # 종가*거래량을 변수로 넣음\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1) # 정확도를 높이기 위해 주가가 오르면 1, 아니면 0으로 설정\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],    # 필요없는 변수들 및 결측값 제거\n",
    "axis=1,\n",
    ").dropna()\n",
    "\n",
    "X = StandardScaler().fit_transform(df.drop([\"y\"], axis=1))    # 딥러닝의 정확도를 높이기 위해 정규화 시행\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(          # 딥러닝의 정확도를 높이기 위해 학습 데이터와 검증 데이터로 나눔\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(2, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.fit(X_train[:, :, np.newaxis], y_train, epochs=50)\n",
    "y_pred = model.predict(X_test[:, :, np.newaxis])\n",
    "print(accuracy_score(y_test, y_pred > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "63/63 [==============================] - 0s 660us/step - loss: 0.6935 - acc: 0.5383\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 676us/step - loss: 0.6942 - acc: 0.5301\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 644us/step - loss: 0.6964 - acc: 0.5015\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 660us/step - loss: 0.6955 - acc: 0.5250\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6961 - acc: 0.5103\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 660us/step - loss: 0.6944 - acc: 0.5068\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6944 - acc: 0.5271\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6925 - acc: 0.5314\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6929 - acc: 0.5268\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6941 - acc: 0.5247\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6937 - acc: 0.5114\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 547us/step - loss: 0.6924 - acc: 0.5308\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 724us/step - loss: 0.6941 - acc: 0.5081\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6920 - acc: 0.5290\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6944 - acc: 0.5006\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6942 - acc: 0.4979\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6923 - acc: 0.5351\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6903 - acc: 0.5296\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6923 - acc: 0.5208\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6939 - acc: 0.5118\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6926 - acc: 0.5186\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 853us/step - loss: 0.6926 - acc: 0.5265\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 676us/step - loss: 0.6933 - acc: 0.5034\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6928 - acc: 0.5285\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6926 - acc: 0.5237\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6928 - acc: 0.5146\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6918 - acc: 0.5343\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6915 - acc: 0.5302\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6925 - acc: 0.5120\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6942 - acc: 0.5045\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6925 - acc: 0.5171\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6912 - acc: 0.5273\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 660us/step - loss: 0.6932 - acc: 0.5050\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6927 - acc: 0.5148\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6930 - acc: 0.5148\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6925 - acc: 0.5151\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6929 - acc: 0.5218\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6926 - acc: 0.5141\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6937 - acc: 0.5049\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6909 - acc: 0.5358\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6902 - acc: 0.5420\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6921 - acc: 0.5234\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6918 - acc: 0.5270\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6920 - acc: 0.5124\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6937 - acc: 0.5010\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6916 - acc: 0.5322\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6923 - acc: 0.5272\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6928 - acc: 0.5146\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6921 - acc: 0.5124\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6931 - acc: 0.5038\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6943 - acc: 0.5033\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6911 - acc: 0.5270\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6928 - acc: 0.5174\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6932 - acc: 0.5038\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6939 - acc: 0.5063\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6920 - acc: 0.5217\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6932 - acc: 0.5023\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6926 - acc: 0.5186\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6909 - acc: 0.5344\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6924 - acc: 0.5173\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6915 - acc: 0.5242\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6912 - acc: 0.5275\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6925 - acc: 0.5130\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6923 - acc: 0.5275\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6922 - acc: 0.5234\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6931 - acc: 0.5087\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6910 - acc: 0.5445\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6922 - acc: 0.5208\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 724us/step - loss: 0.6914 - acc: 0.5292\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6908 - acc: 0.5417\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 676us/step - loss: 0.6920 - acc: 0.5236\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6921 - acc: 0.5200\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6949 - acc: 0.4896\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6929 - acc: 0.5168\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6907 - acc: 0.5323\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6930 - acc: 0.5110\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6944 - acc: 0.4983\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6923 - acc: 0.5194\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6928 - acc: 0.5144\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6915 - acc: 0.5227\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6920 - acc: 0.5231\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6913 - acc: 0.5313\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6919 - acc: 0.5283\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6935 - acc: 0.5093\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 676us/step - loss: 0.6927 - acc: 0.5102\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6927 - acc: 0.5134\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 659us/step - loss: 0.6912 - acc: 0.5387\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6925 - acc: 0.5142\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 676us/step - loss: 0.6915 - acc: 0.5320\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6917 - acc: 0.5169\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6903 - acc: 0.5344\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6907 - acc: 0.5407\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6917 - acc: 0.5232\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6907 - acc: 0.5353\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6924 - acc: 0.5179\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6907 - acc: 0.5363\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6923 - acc: 0.5224\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6921 - acc: 0.5234\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6918 - acc: 0.5240\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6916 - acc: 0.5253\n",
      "0.5487077534791253\n"
     ]
    }
   ],
   "source": [
    "#DNN\n",
    "\n",
    "df = pdr.get_data_yahoo(\"AAPL\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df.Close * df.Volume\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = StandardScaler().fit_transform(df.drop([\"y\"], axis=1))\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "model = Sequential()\n",
    "model.add(Dense(2, activation=\"relu\", input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5487077534791253\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "df = pdr.get_data_yahoo(\"AAPL\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df[\"Close\"] * df[\"Volume\"]\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = df.drop([\"y\"], axis=1).values\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma=\"auto\"))\n",
    "clf.fit(\n",
    "X_train,\n",
    "y_train,\n",
    ")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5487077534791253\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "df = pdr.get_data_yahoo(\"AAPL\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df[\"Close\"] * df[\"Volume\"]\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = df.drop([\"y\"], axis=1).values\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "clf = LogisticRegression()\n",
    "clf.fit(\n",
    "X_train,\n",
    "y_train,\n",
    ")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "63/63 [==============================] - 1s 933us/step - loss: 0.7034 - acc: 0.4908\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.7008 - acc: 0.4985\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.7028 - acc: 0.4549\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 949us/step - loss: 0.6995 - acc: 0.4656\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 853us/step - loss: 0.6968 - acc: 0.4972\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 869us/step - loss: 0.6957 - acc: 0.4890\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6948 - acc: 0.4956\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6960 - acc: 0.5024\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6955 - acc: 0.4940\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6933 - acc: 0.5076\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6947 - acc: 0.4821\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6936 - acc: 0.5039\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6951 - acc: 0.4926\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6948 - acc: 0.4938\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6945 - acc: 0.4909\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6927 - acc: 0.5033\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6932 - acc: 0.5091\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6930 - acc: 0.5189\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6916 - acc: 0.5245\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6924 - acc: 0.5104\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6928 - acc: 0.4984\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6919 - acc: 0.5116\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6918 - acc: 0.5085\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6924 - acc: 0.5227\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6927 - acc: 0.5172\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6917 - acc: 0.5291\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6917 - acc: 0.5249\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6925 - acc: 0.5045\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6929 - acc: 0.5100\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6925 - acc: 0.5150\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6917 - acc: 0.5225\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6923 - acc: 0.5154\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6909 - acc: 0.5074\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6911 - acc: 0.5367\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6909 - acc: 0.5253\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6912 - acc: 0.5391\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6920 - acc: 0.5063\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6914 - acc: 0.5346\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6922 - acc: 0.5155\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6919 - acc: 0.5197\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 853us/step - loss: 0.6925 - acc: 0.5198\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 949us/step - loss: 0.6922 - acc: 0.5140\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6921 - acc: 0.5259\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6935 - acc: 0.4982\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6915 - acc: 0.5110\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6923 - acc: 0.5088\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 949us/step - loss: 0.6928 - acc: 0.5085\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6917 - acc: 0.5226\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6921 - acc: 0.5301\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6916 - acc: 0.5098\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6922 - acc: 0.5100\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6917 - acc: 0.5131\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6919 - acc: 0.5108\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6926 - acc: 0.5113\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.6922 - acc: 0.5228\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6927 - acc: 0.5182\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6908 - acc: 0.5163\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6936 - acc: 0.5166\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6917 - acc: 0.5158\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6912 - acc: 0.5210\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6914 - acc: 0.5127\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6917 - acc: 0.5301\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6915 - acc: 0.5130\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6904 - acc: 0.5362\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6906 - acc: 0.5341\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6907 - acc: 0.5412\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6918 - acc: 0.5198\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6918 - acc: 0.5236\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6923 - acc: 0.5225\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6910 - acc: 0.5218\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6914 - acc: 0.5180\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6924 - acc: 0.5215\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6933 - acc: 0.5132\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6939 - acc: 0.5009\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6904 - acc: 0.5227\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6905 - acc: 0.5228\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 820us/step - loss: 0.6921 - acc: 0.5297\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6904 - acc: 0.5233\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6930 - acc: 0.5284\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6929 - acc: 0.5179\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6923 - acc: 0.5147\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6911 - acc: 0.5254\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 0s 853us/step - loss: 0.6898 - acc: 0.5415\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 804us/step - loss: 0.6935 - acc: 0.4985\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 820us/step - loss: 0.6901 - acc: 0.5470\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6917 - acc: 0.5213\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6914 - acc: 0.5207\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6880 - acc: 0.5572\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6920 - acc: 0.5186\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6908 - acc: 0.5264\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6899 - acc: 0.5383\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 869us/step - loss: 0.6915 - acc: 0.5262\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6911 - acc: 0.5168\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6926 - acc: 0.5197\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6907 - acc: 0.5428\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6914 - acc: 0.5294\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.6924 - acc: 0.5159\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6909 - acc: 0.5210\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 820us/step - loss: 0.6916 - acc: 0.5000\n",
      "0.5924453280318092\n"
     ]
    }
   ],
   "source": [
    "#RNN\n",
    "\n",
    "df = pdr.get_data_yahoo(\"MSFT\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df.Close * df.Volume\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = StandardScaler().fit_transform(df.drop([\"y\"], axis=1))\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(2, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.fit(X_train[:, :, np.newaxis], y_train, epochs=100)\n",
    "y_pred = model.predict(X_test[:, :, np.newaxis])\n",
    "print(accuracy_score(y_test, y_pred > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6936 - acc: 0.5137\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 724us/step - loss: 0.6932 - acc: 0.5179\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6946 - acc: 0.5161\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6941 - acc: 0.5109\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6944 - acc: 0.5198\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6920 - acc: 0.5067\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6929 - acc: 0.5114\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6940 - acc: 0.5113\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6927 - acc: 0.5064\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 660us/step - loss: 0.6928 - acc: 0.5142\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6919 - acc: 0.4922\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6936 - acc: 0.5010\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6910 - acc: 0.5216\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6922 - acc: 0.5121\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6916 - acc: 0.5226\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6902 - acc: 0.5246\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6925 - acc: 0.5050\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6910 - acc: 0.5171\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6917 - acc: 0.5220\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6944 - acc: 0.4982\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 660us/step - loss: 0.6931 - acc: 0.5116\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 660us/step - loss: 0.6940 - acc: 0.5054\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 660us/step - loss: 0.6924 - acc: 0.5169\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6905 - acc: 0.5167\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 660us/step - loss: 0.6919 - acc: 0.5112\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6927 - acc: 0.5157\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6920 - acc: 0.5263\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6929 - acc: 0.5040\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 724us/step - loss: 0.6928 - acc: 0.4882\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6962 - acc: 0.4762\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6934 - acc: 0.4896\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6919 - acc: 0.5191\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6929 - acc: 0.5142\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6912 - acc: 0.5181\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6923 - acc: 0.5063\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 948us/step - loss: 0.6928 - acc: 0.5140\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6930 - acc: 0.5027\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.6912 - acc: 0.5207\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6931 - acc: 0.5040\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 676us/step - loss: 0.6916 - acc: 0.5112\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6892 - acc: 0.5272\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6913 - acc: 0.5024\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6915 - acc: 0.5149\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6909 - acc: 0.5278\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6913 - acc: 0.5047\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6910 - acc: 0.5278\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6906 - acc: 0.5279\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6919 - acc: 0.5193\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 724us/step - loss: 0.6908 - acc: 0.5304\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6896 - acc: 0.5165\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6908 - acc: 0.5157\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6923 - acc: 0.5094\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6923 - acc: 0.5027\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6906 - acc: 0.5263\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6914 - acc: 0.5204\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6918 - acc: 0.5088\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6915 - acc: 0.5100\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6936 - acc: 0.4932\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6931 - acc: 0.4998\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 547us/step - loss: 0.6906 - acc: 0.5138\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6907 - acc: 0.5166\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6911 - acc: 0.5283\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6920 - acc: 0.5071\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6916 - acc: 0.5025\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 676us/step - loss: 0.6917 - acc: 0.5024\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6907 - acc: 0.5280\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 724us/step - loss: 0.6932 - acc: 0.5034\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6904 - acc: 0.5226\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6931 - acc: 0.5023\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 547us/step - loss: 0.6917 - acc: 0.5148\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6917 - acc: 0.5007\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6925 - acc: 0.5096\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6915 - acc: 0.5123\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6922 - acc: 0.5020\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6919 - acc: 0.4963\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6915 - acc: 0.5235\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 660us/step - loss: 0.6913 - acc: 0.5130\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6921 - acc: 0.5017\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6922 - acc: 0.5191\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6905 - acc: 0.5293\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6922 - acc: 0.5070\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6922 - acc: 0.5070\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6920 - acc: 0.5099\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6919 - acc: 0.5156\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 595us/step - loss: 0.6918 - acc: 0.5175\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6910 - acc: 0.5279\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6919 - acc: 0.5111\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6923 - acc: 0.4991\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 547us/step - loss: 0.6931 - acc: 0.5027\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6926 - acc: 0.5149\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6928 - acc: 0.5109\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6917 - acc: 0.5244\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6918 - acc: 0.5154\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6928 - acc: 0.4934\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6903 - acc: 0.5210\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6922 - acc: 0.5000\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6921 - acc: 0.5077\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6901 - acc: 0.5388\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6906 - acc: 0.5093\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6914 - acc: 0.5116\n",
      "0.4552683896620278\n"
     ]
    }
   ],
   "source": [
    "#DNN\n",
    "\n",
    "df = pdr.get_data_yahoo(\"MSFT\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df.Close * df.Volume\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = StandardScaler().fit_transform(df.drop([\"y\"], axis=1))\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "model = Sequential()\n",
    "model.add(Dense(2, activation=\"relu\", input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5288270377733598\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "df = pdr.get_data_yahoo(\"MSFT\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df[\"Close\"] * df[\"Volume\"]\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = df.drop([\"y\"], axis=1).values\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma=\"auto\"))\n",
    "clf.fit(\n",
    "X_train,\n",
    "y_train,\n",
    ")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5765407554671969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "df = pdr.get_data_yahoo(\"MSFT\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df[\"Close\"] * df[\"Volume\"]\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = df.drop([\"y\"], axis=1).values\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "clf = LogisticRegression()\n",
    "clf.fit(\n",
    "X_train,\n",
    "y_train,\n",
    ")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AMZN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "63/63 [==============================] - 1s 901us/step - loss: 0.7007 - acc: 0.5104\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.7046 - acc: 0.4692\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 949us/step - loss: 0.6946 - acc: 0.5127\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6964 - acc: 0.4947\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6917 - acc: 0.5296\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.6916 - acc: 0.5220\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6958 - acc: 0.5247\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6945 - acc: 0.5285\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6934 - acc: 0.5188\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6943 - acc: 0.5280\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.6946 - acc: 0.5311\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.6878 - acc: 0.5510\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6916 - acc: 0.5323\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.6920 - acc: 0.5284\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6910 - acc: 0.5492\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6955 - acc: 0.5105\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6932 - acc: 0.5291\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 949us/step - loss: 0.6927 - acc: 0.5319\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.6914 - acc: 0.5388\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6897 - acc: 0.5291\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6923 - acc: 0.5260\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6904 - acc: 0.5314\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6930 - acc: 0.5135\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6924 - acc: 0.5286\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6902 - acc: 0.5440\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6904 - acc: 0.5339\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6914 - acc: 0.5361\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6919 - acc: 0.5266\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6903 - acc: 0.5320\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6931 - acc: 0.5159\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6969 - acc: 0.5059\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6921 - acc: 0.5316\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 949us/step - loss: 0.6927 - acc: 0.5309\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6896 - acc: 0.5293\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6885 - acc: 0.5407\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6913 - acc: 0.5342\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6916 - acc: 0.5202\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 949us/step - loss: 0.6887 - acc: 0.5394 0s - loss: 0.6885 - acc: 0.540\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6909 - acc: 0.5448\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 853us/step - loss: 0.6905 - acc: 0.5351\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6923 - acc: 0.512 - 0s 981us/step - loss: 0.6920 - acc: 0.5156\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 981us/step - loss: 0.6890 - acc: 0.5453\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6876 - acc: 0.5419\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6900 - acc: 0.5468\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6918 - acc: 0.5289\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6898 - acc: 0.5390\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6906 - acc: 0.5297\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6925 - acc: 0.5141\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 869us/step - loss: 0.6899 - acc: 0.5361\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 869us/step - loss: 0.6907 - acc: 0.5352\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6898 - acc: 0.5421\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6915 - acc: 0.5208\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 869us/step - loss: 0.6878 - acc: 0.5386\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.6922 - acc: 0.5230\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6936 - acc: 0.5084\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6877 - acc: 0.5454\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 869us/step - loss: 0.6868 - acc: 0.5565\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6925 - acc: 0.5153\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 869us/step - loss: 0.6886 - acc: 0.5280\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6906 - acc: 0.5278\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6898 - acc: 0.5296\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6890 - acc: 0.5429\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6898 - acc: 0.5335\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 869us/step - loss: 0.6897 - acc: 0.5323\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6931 - acc: 0.5147\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6882 - acc: 0.5456\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6892 - acc: 0.5391\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6904 - acc: 0.5257\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 869us/step - loss: 0.6896 - acc: 0.5393\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6903 - acc: 0.5385\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 869us/step - loss: 0.6913 - acc: 0.5200\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 981us/step - loss: 0.6892 - acc: 0.5397\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6887 - acc: 0.5454\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6904 - acc: 0.5275\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.6887 - acc: 0.5339\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6890 - acc: 0.5309\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6909 - acc: 0.5161\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 869us/step - loss: 0.6933 - acc: 0.5097\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6910 - acc: 0.5236\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6900 - acc: 0.5292\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6904 - acc: 0.5206\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 853us/step - loss: 0.6924 - acc: 0.5264\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 981us/step - loss: 0.6864 - acc: 0.5544\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6890 - acc: 0.5352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6872 - acc: 0.5465\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6899 - acc: 0.5361\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6912 - acc: 0.5184\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.6894 - acc: 0.5447\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 853us/step - loss: 0.6916 - acc: 0.5213\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 981us/step - loss: 0.6921 - acc: 0.5219\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.6888 - acc: 0.5440\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.6892 - acc: 0.5382\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6862 - acc: 0.5551\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6880 - acc: 0.5430\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.6894 - acc: 0.5351\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.6872 - acc: 0.5378\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 981us/step - loss: 0.6869 - acc: 0.5490\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6880 - acc: 0.5481\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6899 - acc: 0.5405\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6907 - acc: 0.5283\n",
      "0.5467196819085487\n"
     ]
    }
   ],
   "source": [
    "#RNN\n",
    "\n",
    "df = pdr.get_data_yahoo(\"AMZN\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df.Close * df.Volume\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = StandardScaler().fit_transform(df.drop([\"y\"], axis=1))\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(2, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.fit(X_train[:, :, np.newaxis], y_train, epochs=100)\n",
    "y_pred = model.predict(X_test[:, :, np.newaxis])\n",
    "print(accuracy_score(y_test, y_pred > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.7005 - acc: 0.5108\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6985 - acc: 0.5114\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6952 - acc: 0.5119\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6963 - acc: 0.5176\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 724us/step - loss: 0.6941 - acc: 0.5157\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 676us/step - loss: 0.6944 - acc: 0.5090\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6933 - acc: 0.5303\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6933 - acc: 0.5279\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 724us/step - loss: 0.6926 - acc: 0.5279\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 659us/step - loss: 0.6914 - acc: 0.5278\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 820us/step - loss: 0.6899 - acc: 0.5408\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 676us/step - loss: 0.6935 - acc: 0.5166\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 660us/step - loss: 0.6896 - acc: 0.5532\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 724us/step - loss: 0.6894 - acc: 0.5465\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6924 - acc: 0.5334\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6911 - acc: 0.5362\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 676us/step - loss: 0.6914 - acc: 0.5287\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6927 - acc: 0.5121\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6904 - acc: 0.5340\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 853us/step - loss: 0.6927 - acc: 0.5194\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6876 - acc: 0.5530\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6924 - acc: 0.5241\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6891 - acc: 0.5460\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6911 - acc: 0.5263\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6917 - acc: 0.5202\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6895 - acc: 0.5296\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6913 - acc: 0.5302\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 660us/step - loss: 0.6895 - acc: 0.5413\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6925 - acc: 0.5305\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 676us/step - loss: 0.6921 - acc: 0.5198\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6896 - acc: 0.5377\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 676us/step - loss: 0.6892 - acc: 0.5334\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 676us/step - loss: 0.6896 - acc: 0.5358\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6915 - acc: 0.5198\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6882 - acc: 0.5494\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6880 - acc: 0.5368\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6906 - acc: 0.5285\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6901 - acc: 0.5314\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6892 - acc: 0.5384\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6920 - acc: 0.5309\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6928 - acc: 0.5150\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6888 - acc: 0.5383\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6908 - acc: 0.5233\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6914 - acc: 0.5260\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6892 - acc: 0.5374\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6903 - acc: 0.5319\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6892 - acc: 0.5459\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6943 - acc: 0.5116\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6888 - acc: 0.5417\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6917 - acc: 0.5330\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6899 - acc: 0.5415\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6924 - acc: 0.5275\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6905 - acc: 0.5249\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6915 - acc: 0.5218\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6902 - acc: 0.5295\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6894 - acc: 0.5271\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6896 - acc: 0.5372\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6905 - acc: 0.5278\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6899 - acc: 0.5428\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6909 - acc: 0.5279\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6880 - acc: 0.5410\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6898 - acc: 0.5289\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6915 - acc: 0.5271\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6916 - acc: 0.5330\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6896 - acc: 0.5361\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6936 - acc: 0.5214\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6886 - acc: 0.5345\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6922 - acc: 0.5177\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6911 - acc: 0.5216\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6891 - acc: 0.5280\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6914 - acc: 0.5340\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6905 - acc: 0.5391\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6909 - acc: 0.5148\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6888 - acc: 0.5430\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6920 - acc: 0.5200\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6911 - acc: 0.5366\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6866 - acc: 0.5566\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6917 - acc: 0.5240\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6880 - acc: 0.5482\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6885 - acc: 0.5406\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6868 - acc: 0.5437\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6912 - acc: 0.5251\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6773 - acc: 0.562 - 0s 804us/step - loss: 0.6913 - acc: 0.5264\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 852us/step - loss: 0.6898 - acc: 0.5265\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6911 - acc: 0.5364\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 547us/step - loss: 0.6894 - acc: 0.5356\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6870 - acc: 0.5441\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6874 - acc: 0.5409\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6895 - acc: 0.5326\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6903 - acc: 0.5302\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6896 - acc: 0.5308\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6930 - acc: 0.5213\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6897 - acc: 0.5325\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6882 - acc: 0.5328\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6927 - acc: 0.5129\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6896 - acc: 0.5356\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6903 - acc: 0.5275\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6893 - acc: 0.5428\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6879 - acc: 0.5309\n",
      "0.5427435387673957\n"
     ]
    }
   ],
   "source": [
    "#DNN\n",
    "\n",
    "df = pdr.get_data_yahoo(\"AMZN\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df.Close * df.Volume\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = StandardScaler().fit_transform(df.drop([\"y\"], axis=1))\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "model = Sequential()\n",
    "model.add(Dense(2, activation=\"relu\", input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5228628230616302\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "df = pdr.get_data_yahoo(\"AMZN\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df[\"Close\"] * df[\"Volume\"]\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = df.drop([\"y\"], axis=1).values\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma=\"auto\"))\n",
    "clf.fit(\n",
    "X_train,\n",
    "y_train,\n",
    ")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5427435387673957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "df = pdr.get_data_yahoo(\"AMZN\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df[\"Close\"] * df[\"Volume\"]\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = df.drop([\"y\"], axis=1).values\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "clf = LogisticRegression()\n",
    "clf.fit(\n",
    "X_train,\n",
    "y_train,\n",
    ")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOOGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "63/63 [==============================] - 1s 965us/step - loss: 0.6950 - acc: 0.5161\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6952 - acc: 0.5090\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6938 - acc: 0.5052\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6925 - acc: 0.5199\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6923 - acc: 0.5302\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6937 - acc: 0.5114\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6932 - acc: 0.5165\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6943 - acc: 0.5053\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6925 - acc: 0.5199\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6917 - acc: 0.5269\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6922 - acc: 0.5303\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.6934 - acc: 0.5186\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6926 - acc: 0.5275\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6944 - acc: 0.5092\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6927 - acc: 0.5221\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6915 - acc: 0.5294\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.6916 - acc: 0.5342\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6924 - acc: 0.5269\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6902 - acc: 0.5421\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6938 - acc: 0.5251\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6899 - acc: 0.5541\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6935 - acc: 0.5222\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6910 - acc: 0.5286\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6897 - acc: 0.5430\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6922 - acc: 0.5314\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6901 - acc: 0.5366\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6937 - acc: 0.5184\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6928 - acc: 0.5153\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6915 - acc: 0.5305\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6911 - acc: 0.5368\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6915 - acc: 0.5229\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6916 - acc: 0.5280\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6939 - acc: 0.5120\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6912 - acc: 0.5313\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 853us/step - loss: 0.6911 - acc: 0.5334\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6924 - acc: 0.5287\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6913 - acc: 0.5326\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 885us/step - loss: 0.6884 - acc: 0.5547\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6911 - acc: 0.5313\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 853us/step - loss: 0.6923 - acc: 0.5302\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6906 - acc: 0.5320\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.6901 - acc: 0.5362\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6904 - acc: 0.5332\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6916 - acc: 0.5306\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 949us/step - loss: 0.6912 - acc: 0.5376\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6922 - acc: 0.5270\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6907 - acc: 0.5450\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6927 - acc: 0.5205\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6921 - acc: 0.5200\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6895 - acc: 0.5451\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6909 - acc: 0.5381\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6920 - acc: 0.5246\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6907 - acc: 0.5327\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6926 - acc: 0.5219\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6920 - acc: 0.5244\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6887 - acc: 0.5531\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6932 - acc: 0.5255\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.6899 - acc: 0.5418\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6914 - acc: 0.5330\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6917 - acc: 0.5232\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6911 - acc: 0.5327\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6919 - acc: 0.5269\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6912 - acc: 0.5385\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6898 - acc: 0.5469\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6921 - acc: 0.5191\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6901 - acc: 0.5353\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6902 - acc: 0.5382\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6916 - acc: 0.5193\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6890 - acc: 0.5492\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6877 - acc: 0.5573\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6924 - acc: 0.5267\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6943 - acc: 0.5025\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6924 - acc: 0.5253\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6916 - acc: 0.5313\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6916 - acc: 0.5288\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6913 - acc: 0.5267\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6919 - acc: 0.5309\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 853us/step - loss: 0.6903 - acc: 0.5376\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6935 - acc: 0.5216\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6912 - acc: 0.5343\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6911 - acc: 0.5413\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 836us/step - loss: 0.6904 - acc: 0.5371\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6924 - acc: 0.5253\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6900 - acc: 0.5360\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 756us/step - loss: 0.6892 - acc: 0.5473\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6918 - acc: 0.5260\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6902 - acc: 0.5407\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6929 - acc: 0.5211\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6883 - acc: 0.5523\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6916 - acc: 0.5286\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6926 - acc: 0.5186\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6921 - acc: 0.5229\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6907 - acc: 0.5344\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6917 - acc: 0.5210\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6903 - acc: 0.5373\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6926 - acc: 0.5231\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 804us/step - loss: 0.6920 - acc: 0.5265\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6902 - acc: 0.5377\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6917 - acc: 0.5258\n",
      "0.4990059642147117\n"
     ]
    }
   ],
   "source": [
    "#RNN\n",
    "\n",
    "df = pdr.get_data_yahoo(\"GOOGL\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df.Close * df.Volume\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = StandardScaler().fit_transform(df.drop([\"y\"], axis=1))\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(2, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.fit(X_train[:, :, np.newaxis], y_train, epochs=100)\n",
    "y_pred = model.predict(X_test[:, :, np.newaxis])\n",
    "print(accuracy_score(y_test, y_pred > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "63/63 [==============================] - 0s 676us/step - loss: 0.6975 - acc: 0.4969\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6951 - acc: 0.5055\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6936 - acc: 0.5203\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 676us/step - loss: 0.6913 - acc: 0.5301\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6944 - acc: 0.5091\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6899 - acc: 0.5495\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6923 - acc: 0.5173\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 660us/step - loss: 0.6943 - acc: 0.5249\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6926 - acc: 0.5246\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6913 - acc: 0.5302\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6914 - acc: 0.5396\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6913 - acc: 0.5284\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6924 - acc: 0.5219\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6949 - acc: 0.5252\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6880 - acc: 0.5493\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6903 - acc: 0.5395\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6927 - acc: 0.5186\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6898 - acc: 0.5453\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6915 - acc: 0.5272\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6921 - acc: 0.5296\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 788us/step - loss: 0.6912 - acc: 0.5259\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6928 - acc: 0.5251\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6912 - acc: 0.5294\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 916us/step - loss: 0.6920 - acc: 0.5171\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 820us/step - loss: 0.6895 - acc: 0.5404\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6916 - acc: 0.5305\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 724us/step - loss: 0.6920 - acc: 0.5212\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6909 - acc: 0.5314\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6901 - acc: 0.5343\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6912 - acc: 0.5343\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6931 - acc: 0.5244\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6914 - acc: 0.5426\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6899 - acc: 0.5459\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 676us/step - loss: 0.6901 - acc: 0.5368\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 660us/step - loss: 0.6931 - acc: 0.5225\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6929 - acc: 0.5184\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6895 - acc: 0.5496\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 660us/step - loss: 0.6909 - acc: 0.5380\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6904 - acc: 0.5357\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6925 - acc: 0.5279\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 869us/step - loss: 0.6917 - acc: 0.5226\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6896 - acc: 0.5363\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6942 - acc: 0.5113\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 676us/step - loss: 0.6915 - acc: 0.5210\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6929 - acc: 0.5202\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6916 - acc: 0.5273\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6906 - acc: 0.5312\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6918 - acc: 0.5227\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6921 - acc: 0.5243\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 676us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6936 - acc: 0.5104\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6895 - acc: 0.5400\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6903 - acc: 0.5355\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6911 - acc: 0.5251\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 724us/step - loss: 0.6923 - acc: 0.5201\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6888 - acc: 0.5417\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6908 - acc: 0.5308\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6921 - acc: 0.5186\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6902 - acc: 0.5353\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 692us/step - loss: 0.6916 - acc: 0.5240\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6913 - acc: 0.5165\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6915 - acc: 0.5323\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6907 - acc: 0.5379\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 772us/step - loss: 0.6920 - acc: 0.5250\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6904 - acc: 0.5473\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 724us/step - loss: 0.6914 - acc: 0.5290\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 708us/step - loss: 0.6895 - acc: 0.5501\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 724us/step - loss: 0.6905 - acc: 0.5347\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6911 - acc: 0.5333\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 724us/step - loss: 0.6911 - acc: 0.5375\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6899 - acc: 0.5331\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6920 - acc: 0.5217\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 917us/step - loss: 0.6912 - acc: 0.5314\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 0s 901us/step - loss: 0.6904 - acc: 0.5365\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 740us/step - loss: 0.6915 - acc: 0.5284\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.6931 - acc: 0.5144\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6940 - acc: 0.5068\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6889 - acc: 0.5479\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6921 - acc: 0.5229\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6911 - acc: 0.5370\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6911 - acc: 0.5268\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6909 - acc: 0.5296\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 820us/step - loss: 0.6909 - acc: 0.5328\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 0s 853us/step - loss: 0.6889 - acc: 0.5410\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 563us/step - loss: 0.6894 - acc: 0.5465\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6914 - acc: 0.5228\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6919 - acc: 0.5277\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 643us/step - loss: 0.6908 - acc: 0.5365\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 531us/step - loss: 0.6927 - acc: 0.5109\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 627us/step - loss: 0.6912 - acc: 0.5256\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6902 - acc: 0.5356\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.6897 - acc: 0.5368\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6924 - acc: 0.5176\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6921 - acc: 0.5203\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 547us/step - loss: 0.6888 - acc: 0.5449\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 579us/step - loss: 0.6907 - acc: 0.5281\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6901 - acc: 0.5374\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 611us/step - loss: 0.6897 - acc: 0.5301\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6897 - acc: 0.5398\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.6931 - acc: 0.5169\n",
      "0.5427435387673957\n"
     ]
    }
   ],
   "source": [
    "#DNN\n",
    "\n",
    "df = pdr.get_data_yahoo(\"GOOGL\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df.Close * df.Volume\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = StandardScaler().fit_transform(df.drop([\"y\"], axis=1))\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "model = Sequential()\n",
    "model.add(Dense(2, activation=\"relu\", input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48906560636182905\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "df = pdr.get_data_yahoo(\"GOOGL\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df[\"Close\"] * df[\"Volume\"]\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = df.drop([\"y\"], axis=1).values\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma=\"auto\"))\n",
    "clf.fit(\n",
    "X_train,\n",
    "y_train,\n",
    ")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5467196819085487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "df = pdr.get_data_yahoo(\"GOOGL\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df[\"Close\"] * df[\"Volume\"]\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = df.drop([\"y\"], axis=1).values\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "clf = LogisticRegression()\n",
    "clf.fit(\n",
    "X_train,\n",
    "y_train,\n",
    ")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "54/54 [==============================] - 1s 941us/step - loss: 0.6984 - acc: 0.5182\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6960 - acc: 0.5138\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 0s 828us/step - loss: 0.6929 - acc: 0.5288\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 0s 884us/step - loss: 0.6947 - acc: 0.5070\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 0s 790us/step - loss: 0.6968 - acc: 0.4935\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 0s 809us/step - loss: 0.6931 - acc: 0.5069\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 0s 866us/step - loss: 0.6916 - acc: 0.5247\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 0s 847us/step - loss: 0.6931 - acc: 0.5199\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 0s 828us/step - loss: 0.6923 - acc: 0.5165\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 0s 941us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 0s 790us/step - loss: 0.6915 - acc: 0.5287\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 0s 753us/step - loss: 0.6918 - acc: 0.5250\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 0s 866us/step - loss: 0.6902 - acc: 0.5357\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 0s 828us/step - loss: 0.6936 - acc: 0.4982\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 0s 847us/step - loss: 0.6919 - acc: 0.5150\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 0s 809us/step - loss: 0.6917 - acc: 0.5212\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 0s 790us/step - loss: 0.6909 - acc: 0.5425\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 0s 941us/step - loss: 0.6927 - acc: 0.5126\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 0s 847us/step - loss: 0.6893 - acc: 0.5540\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 0s 997us/step - loss: 0.6923 - acc: 0.5140\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 0s 809us/step - loss: 0.6901 - acc: 0.5356\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 0s 790us/step - loss: 0.6906 - acc: 0.5339\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 0s 922us/step - loss: 0.6926 - acc: 0.5141\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 0s 847us/step - loss: 0.6925 - acc: 0.5211\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 0s 979us/step - loss: 0.6913 - acc: 0.5298\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6917 - acc: 0.5161\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 0s 922us/step - loss: 0.6898 - acc: 0.5411\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 0s 847us/step - loss: 0.6911 - acc: 0.5330\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 0s 979us/step - loss: 0.6912 - acc: 0.5276\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6915 - acc: 0.5196\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 0s 997us/step - loss: 0.6911 - acc: 0.5210\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 0s 941us/step - loss: 0.6890 - acc: 0.5452\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 0s 809us/step - loss: 0.6921 - acc: 0.5140\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 0s 866us/step - loss: 0.6905 - acc: 0.5355\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 0s 903us/step - loss: 0.6913 - acc: 0.5214\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 0s 847us/step - loss: 0.6881 - acc: 0.5630\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 0s 922us/step - loss: 0.6915 - acc: 0.5262\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6925 - acc: 0.5175\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6923 - acc: 0.5165\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 0s 903us/step - loss: 0.6908 - acc: 0.5219\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 0s 828us/step - loss: 0.6918 - acc: 0.5256\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 0s 828us/step - loss: 0.6901 - acc: 0.5278\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 0s 828us/step - loss: 0.6901 - acc: 0.5311\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 0s 809us/step - loss: 0.6923 - acc: 0.5129\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 0s 809us/step - loss: 0.6907 - acc: 0.5300\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 0s 941us/step - loss: 0.6895 - acc: 0.5420\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6913 - acc: 0.5290\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 0s 997us/step - loss: 0.6913 - acc: 0.5260\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 0s 922us/step - loss: 0.6897 - acc: 0.5408\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 0s 903us/step - loss: 0.6913 - acc: 0.5306\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 0s 828us/step - loss: 0.6906 - acc: 0.5309\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 0s 922us/step - loss: 0.6896 - acc: 0.5385\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 0s 847us/step - loss: 0.6871 - acc: 0.5686\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 0s 884us/step - loss: 0.6895 - acc: 0.5447\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 0s 790us/step - loss: 0.6891 - acc: 0.5344\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 0s 866us/step - loss: 0.6896 - acc: 0.5462\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 0s 772us/step - loss: 0.6917 - acc: 0.5207\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 0s 809us/step - loss: 0.6888 - acc: 0.5550\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 0s 790us/step - loss: 0.6925 - acc: 0.5149\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 0s 771us/step - loss: 0.6895 - acc: 0.5333\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 0s 847us/step - loss: 0.6900 - acc: 0.5433\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 0s 790us/step - loss: 0.6898 - acc: 0.5365\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 0s 903us/step - loss: 0.6903 - acc: 0.5399\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 0s 866us/step - loss: 0.6900 - acc: 0.5340\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 0s 753us/step - loss: 0.6906 - acc: 0.5366\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 0s 847us/step - loss: 0.6914 - acc: 0.5286\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 0s 809us/step - loss: 0.6895 - acc: 0.5320\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 0s 866us/step - loss: 0.6915 - acc: 0.5232\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 0s 884us/step - loss: 0.6900 - acc: 0.5338\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 0s 847us/step - loss: 0.6903 - acc: 0.5338\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6897 - acc: 0.5357\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 0s 941us/step - loss: 0.6923 - acc: 0.5328\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 0s 809us/step - loss: 0.6925 - acc: 0.5149\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 0s 828us/step - loss: 0.6886 - acc: 0.5469\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 0s 828us/step - loss: 0.6905 - acc: 0.5343\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 0s 790us/step - loss: 0.6910 - acc: 0.5192\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 0s 866us/step - loss: 0.6920 - acc: 0.5242\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6902 - acc: 0.5296\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6915 - acc: 0.5227\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 0s 941us/step - loss: 0.6898 - acc: 0.5350\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 0s 866us/step - loss: 0.6899 - acc: 0.5315\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 0s 828us/step - loss: 0.6902 - acc: 0.5193\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 0s 922us/step - loss: 0.6907 - acc: 0.5331\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 0s 828us/step - loss: 0.6877 - acc: 0.5483\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 753us/step - loss: 0.6904 - acc: 0.5256\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 0s 772us/step - loss: 0.6913 - acc: 0.5204\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 0s 809us/step - loss: 0.6921 - acc: 0.5201\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 0s 828us/step - loss: 0.6894 - acc: 0.5391\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 0s 809us/step - loss: 0.6916 - acc: 0.5278\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 0s 866us/step - loss: 0.6905 - acc: 0.5306\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 0s 866us/step - loss: 0.6904 - acc: 0.5355\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 0s 978us/step - loss: 0.6897 - acc: 0.5319\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 0s 866us/step - loss: 0.6891 - acc: 0.5364\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 0s 903us/step - loss: 0.6899 - acc: 0.5368\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 0s 866us/step - loss: 0.6912 - acc: 0.5247\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 0s 828us/step - loss: 0.6909 - acc: 0.5287\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 0s 922us/step - loss: 0.6930 - acc: 0.5156\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 0s 903us/step - loss: 0.6897 - acc: 0.5304\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 0s 903us/step - loss: 0.6883 - acc: 0.5459\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 0s 997us/step - loss: 0.6905 - acc: 0.5230\n",
      "0.5435294117647059\n"
     ]
    }
   ],
   "source": [
    "#RNN\n",
    "\n",
    "df = pdr.get_data_yahoo(\"FB\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df.Close * df.Volume\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = StandardScaler().fit_transform(df.drop([\"y\"], axis=1))\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(2, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.fit(X_train[:, :, np.newaxis], y_train, epochs=100)\n",
    "y_pred = model.predict(X_test[:, :, np.newaxis])\n",
    "print(accuracy_score(y_test, y_pred > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "54/54 [==============================] - 0s 715us/step - loss: 0.8120 - acc: 0.5054\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 0s 809us/step - loss: 0.7784 - acc: 0.5206\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 0s 771us/step - loss: 0.7500 - acc: 0.5120\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 0s 734us/step - loss: 0.7590 - acc: 0.4950\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 0s 678us/step - loss: 0.7311 - acc: 0.5198\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 0s 677us/step - loss: 0.7300 - acc: 0.4918\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 0s 696us/step - loss: 0.7146 - acc: 0.5069\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.7284 - acc: 0.4860\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 0s 677us/step - loss: 0.7170 - acc: 0.4898\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 0s 659us/step - loss: 0.7192 - acc: 0.4958\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 0s 659us/step - loss: 0.7095 - acc: 0.4735\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 0s 659us/step - loss: 0.7173 - acc: 0.4903\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.7148 - acc: 0.4822\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.7094 - acc: 0.4775\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.7128 - acc: 0.4736\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.7048 - acc: 0.4875\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.7056 - acc: 0.4807\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.7084 - acc: 0.4625\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.7016 - acc: 0.4939\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.7063 - acc: 0.4742\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 0s 583us/step - loss: 0.7081 - acc: 0.4767\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 0s 715us/step - loss: 0.7023 - acc: 0.4776\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 0s 734us/step - loss: 0.7053 - acc: 0.4751\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 0s 696us/step - loss: 0.7019 - acc: 0.4749\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.7022 - acc: 0.4755\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 0s 772us/step - loss: 0.7022 - acc: 0.4633\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 0s 828us/step - loss: 0.6976 - acc: 0.4770\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 0s 809us/step - loss: 0.7000 - acc: 0.4771\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.6988 - acc: 0.5072\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.6977 - acc: 0.4880\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.6985 - acc: 0.4968\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.6982 - acc: 0.4783\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 0s 677us/step - loss: 0.6962 - acc: 0.5016\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 0s 677us/step - loss: 0.6964 - acc: 0.5002\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 0s 734us/step - loss: 0.6951 - acc: 0.4875\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.6983 - acc: 0.4965\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 0s 583us/step - loss: 0.6946 - acc: 0.5160\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 0s 583us/step - loss: 0.6970 - acc: 0.4939\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 0s 659us/step - loss: 0.6949 - acc: 0.5095\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.6964 - acc: 0.5207\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.6952 - acc: 0.5202\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 0s 583us/step - loss: 0.6962 - acc: 0.5108\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.6946 - acc: 0.5125\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.6930 - acc: 0.5354\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.6952 - acc: 0.5146\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.6929 - acc: 0.5314\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.6940 - acc: 0.5216\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 0s 696us/step - loss: 0.6932 - acc: 0.5252\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.6928 - acc: 0.5372\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 0s 734us/step - loss: 0.6916 - acc: 0.5443\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 0s 715us/step - loss: 0.6918 - acc: 0.5247\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 0s 696us/step - loss: 0.6929 - acc: 0.5273\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 0s 677us/step - loss: 0.6922 - acc: 0.5296\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.6934 - acc: 0.5148\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.6924 - acc: 0.5228\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 0s 659us/step - loss: 0.6933 - acc: 0.5158\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 0s 772us/step - loss: 0.6926 - acc: 0.5258\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.6913 - acc: 0.5388\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 0s 771us/step - loss: 0.6919 - acc: 0.5181\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.6927 - acc: 0.5186\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.6927 - acc: 0.5179\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.6921 - acc: 0.5225\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 0s 583us/step - loss: 0.6918 - acc: 0.5241\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 0s 565us/step - loss: 0.6921 - acc: 0.5240\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 0s 809us/step - loss: 0.6942 - acc: 0.5032\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.6900 - acc: 0.5461\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.6928 - acc: 0.5154\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.6929 - acc: 0.5199\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.6918 - acc: 0.5199\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.6939 - acc: 0.5013\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 0s 696us/step - loss: 0.6923 - acc: 0.5226\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.6914 - acc: 0.5355\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 0s 772us/step - loss: 0.6910 - acc: 0.5335\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 0s 677us/step - loss: 0.6925 - acc: 0.5233\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.6919 - acc: 0.5250\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.6919 - acc: 0.5254\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 0s 734us/step - loss: 0.6910 - acc: 0.5321\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 0s 659us/step - loss: 0.6926 - acc: 0.5213\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 0s 790us/step - loss: 0.6921 - acc: 0.5285\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 0s 828us/step - loss: 0.6930 - acc: 0.5071\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 0s 809us/step - loss: 0.6904 - acc: 0.5434\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 0s 659us/step - loss: 0.6927 - acc: 0.5138\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.6917 - acc: 0.5319\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.6941 - acc: 0.5005\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 602us/step - loss: 0.6930 - acc: 0.5134\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.6916 - acc: 0.5362\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.6915 - acc: 0.5257\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.6913 - acc: 0.5318\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.6919 - acc: 0.5257\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 0s 583us/step - loss: 0.6921 - acc: 0.5213\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.6899 - acc: 0.5496\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 0s 565us/step - loss: 0.6908 - acc: 0.5431\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.6908 - acc: 0.5329\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 0s 583us/step - loss: 0.6948 - acc: 0.4996\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 0s 583us/step - loss: 0.6928 - acc: 0.5215\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 0s 659us/step - loss: 0.6913 - acc: 0.5332\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.6938 - acc: 0.5144\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.6930 - acc: 0.5160\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.6913 - acc: 0.5406\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.6900 - acc: 0.5447\n",
      "0.5458823529411765\n"
     ]
    }
   ],
   "source": [
    "#DNN\n",
    "\n",
    "df = pdr.get_data_yahoo(\"FB\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df.Close * df.Volume\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = StandardScaler().fit_transform(df.drop([\"y\"], axis=1))\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "model = Sequential()\n",
    "model.add(Dense(2, activation=\"relu\", input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "df = pdr.get_data_yahoo(\"FB\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df[\"Close\"] * df[\"Volume\"]\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = df.drop([\"y\"], axis=1).values\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma=\"auto\"))\n",
    "clf.fit(\n",
    "X_train,\n",
    "y_train,\n",
    ")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5458823529411765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "df = pdr.get_data_yahoo(\"FB\", \"2010-11-01\", \"2020-11-01\")\n",
    "df[\"Diff\"] = df.Close.diff()\n",
    "df[\"SMA_2\"] = df.Close.rolling(5).mean()\n",
    "df[\"Force_Index\"] = df[\"Close\"] * df[\"Volume\"]\n",
    "df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
    "df = df.drop(\n",
    "[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Diff\", \"Adj Close\"],\n",
    "axis=1,\n",
    ").dropna()\n",
    "# print(df)\n",
    "X = df.drop([\"y\"], axis=1).values\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X,\n",
    "y,\n",
    "test_size=0.2,\n",
    "shuffle=False,\n",
    ")\n",
    "clf = LogisticRegression()\n",
    "clf.fit(\n",
    "X_train,\n",
    "y_train,\n",
    ")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
